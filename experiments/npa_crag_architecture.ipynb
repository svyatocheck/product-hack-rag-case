{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad855fb-4fb3-4773-9f69-dfc01844991d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain_community tiktoken langchain-openai langchainhub langchain langgraph langchain_huggingface\n",
    "%pip install -U pymilvus sentence-transformers fsspec s3fs yandexcloud transformers pip install langchain-experimental\n",
    "%pip install --upgrade huggingface-hub\n",
    "%pip install -qU  langchain_milvus\n",
    "%pip install pydantic==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ef898-0859-4488-aee1-00c4a90cd720",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Yandex Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5afa18-330a-413a-a10b-c8c62aa0b785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:00.571785Z",
     "iopub.status.busy": "2024-09-12T20:11:00.571265Z",
     "iopub.status.idle": "2024-09-12T20:11:00.600454Z",
     "shell.execute_reply": "2024-09-12T20:11:00.599636Z",
     "shell.execute_reply.started": "2024-09-12T20:11:00.571748Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set keys here\n",
    "service_account_id = \"\"\n",
    "key_id = \"\"\n",
    "private_key = \"\"\"\n",
    "PLEASE DO NOT REMOVE THIS LINE! Yandex.Cloud SA Key ID <ajeinv58ca4aeosoqncn>\n",
    "-----BEGIN PRIVATE KEY-----\n",
    "\n",
    "-----END PRIVATE KEY-----\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e13716-438c-408b-b13e-11c3f9c6086e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:00.604235Z",
     "iopub.status.busy": "2024-09-12T20:11:00.602664Z",
     "iopub.status.idle": "2024-09-12T20:11:01.022428Z",
     "shell.execute_reply": "2024-09-12T20:11:01.021764Z",
     "shell.execute_reply.started": "2024-09-12T20:11:00.604196Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare yandex gpt model for connections\n",
    "import time\n",
    "\n",
    "import jwt\n",
    "import requests\n",
    "\n",
    "# Получаем IAM-токен\n",
    "\n",
    "now = int(time.time())\n",
    "payload = {\n",
    "    \"aud\": \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "    \"iss\": service_account_id,\n",
    "    \"iat\": now,\n",
    "    \"exp\": now + 3600,\n",
    "}\n",
    "\n",
    "\n",
    "# Формирование JWT.\n",
    "encoded_token = jwt.encode(\n",
    "    payload, private_key, algorithm=\"PS256\", headers={\"kid\": key_id}\n",
    ")\n",
    "\n",
    "\n",
    "# Запись ключа в файл\n",
    "with open(\"jwt_token.txt\", \"w\") as j:\n",
    "    j.write(encoded_token)\n",
    "\n",
    "# Вывод в консоль\n",
    "print(encoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b815f9-472f-482e-8a70-cf50470010b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:01.027731Z",
     "iopub.status.busy": "2024-09-12T20:11:01.024617Z",
     "iopub.status.idle": "2024-09-12T20:11:01.113851Z",
     "shell.execute_reply": "2024-09-12T20:11:01.113236Z",
     "shell.execute_reply.started": "2024-09-12T20:11:01.027676Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_iam_token():\n",
    "    now = int(time.time())\n",
    "    payload = {\n",
    "        \"aud\": \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "        \"iss\": service_account_id,\n",
    "        \"iat\": now,\n",
    "        \"exp\": now + 3600,\n",
    "    }\n",
    "    encoded_token = jwt.encode(\n",
    "        payload, private_key, algorithm=\"PS256\", headers={\"kid\": key_id}\n",
    "    )\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json={\"jwt\": encoded_token},\n",
    "    )\n",
    "\n",
    "    return response.json()[\"iamToken\"]\n",
    "\n",
    "\n",
    "# Получаем IAM-токен\n",
    "iam_token = get_iam_token()\n",
    "\n",
    "# Печатаем IAM-токен для отладки\n",
    "print(\"IAM Token:\", iam_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0404ba60-a38c-4db9-b505-9c3f20c5ac94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:01.117116Z",
     "iopub.status.busy": "2024-09-12T20:11:01.116259Z",
     "iopub.status.idle": "2024-09-12T20:11:01.128328Z",
     "shell.execute_reply": "2024-09-12T20:11:01.127471Z",
     "shell.execute_reply.started": "2024-09-12T20:11:01.117080Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_id = 'b1g5pkij4170jm1a1un6'\n",
    "filename = \"npa_dataset_v4.db\"  # change dataset version when new iteration begins!\n",
    "database_path = f\"/home/jupyter/datasphere/s3/hack-object-storage/database/{filename}\"\n",
    "dataset_path = (\n",
    "    \"/home/jupyter/datasphere/datasets/npa_two/preprocessed_dataset_ver_2.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e89fa-8b97-4128-8150-161c313c6045",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6789a784-d158-4bfe-b20e-ce42503aead9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:01.129706Z",
     "iopub.status.busy": "2024-09-12T20:11:01.129237Z",
     "iopub.status.idle": "2024-09-12T20:11:01.147945Z",
     "shell.execute_reply": "2024-09-12T20:11:01.147237Z",
     "shell.execute_reply.started": "2024-09-12T20:11:01.129674Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(dataset_path, encoding=\"utf-8\") as file:\n",
    "#     dataset_text = file.read()\n",
    "\n",
    "# data_raw = dataset_text.split(\"</s>\\n\")\n",
    "# data_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c9a64b-a90a-46f8-9083-ec211853ebd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:01.149536Z",
     "iopub.status.busy": "2024-09-12T20:11:01.148989Z",
     "iopub.status.idle": "2024-09-12T20:11:01.161774Z",
     "shell.execute_reply": "2024-09-12T20:11:01.161185Z",
     "shell.execute_reply.started": "2024-09-12T20:11:01.149504Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# documents = []\n",
    "# for texts in data_raw:\n",
    "#     doc = Document(page_content=texts, metadata={\"source\": dataset_path, \"database_path\": database_path})\n",
    "#     documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391146b4-87be-40f0-beb1-4606d04509e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:01.163716Z",
     "iopub.status.busy": "2024-09-12T20:11:01.162519Z",
     "iopub.status.idle": "2024-09-12T20:11:01.176459Z",
     "shell.execute_reply": "2024-09-12T20:11:01.175910Z",
     "shell.execute_reply.started": "2024-09-12T20:11:01.163684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_milvus import Milvus\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from transformers import AutoModel\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"deepvk/USER-bge-m3\", trust_remote_code=True) \n",
    "\n",
    "# model_name = \"deepvk/USER-bge-m3\"\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs=model_kwargs,\n",
    "# )\n",
    "\n",
    "# # Retriever\n",
    "# # API Here: https://python.langchain.com/v0.2/api_reference/milvus/vectorstores/langchain_milvus.vectorstores.milvus.Milvus.html\n",
    "# vectorstore = Milvus.from_documents(\n",
    "#     documents = documents[:100],\n",
    "#     collection_name = \"npa_storage_512_64\", \n",
    "#     connection_args={\"uri\": database_path},\n",
    "#     embedding = embeddings\n",
    "# )\n",
    "\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1aab5-74ca-4f7e-8881-1151bfbdabcf",
   "metadata": {},
   "source": [
    "## Prepare Self-Reflective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05342e5e-e2d3-41e6-8de6-100b1a83cce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:01.178020Z",
     "iopub.status.busy": "2024-09-12T20:11:01.177218Z",
     "iopub.status.idle": "2024-09-12T20:11:43.745239Z",
     "shell.execute_reply": "2024-09-12T20:11:43.744377Z",
     "shell.execute_reply.started": "2024-09-12T20:11:01.177987Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"deepvk/USER-bge-m3\", trust_remote_code=True) \n",
    "\n",
    "model_name = \"deepvk/USER-bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "# Retriever\n",
    "# API Here: https://python.langchain.com/v0.2/api_reference/milvus/vectorstores/langchain_milvus.vectorstores.milvus.Milvus.html\n",
    "vectorstore = Milvus(\n",
    "    collection_name = \"npa_storage_1024_128\", \n",
    "    connection_args={\"uri\": database_path},\n",
    "    embedding_function = embeddings\n",
    ")\n",
    "_\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20, \"fetch_k\": 50, \"lambda_mult\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff20021-d7b0-4c7d-89a6-fc410053f0e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:43.751200Z",
     "iopub.status.busy": "2024-09-12T20:11:43.750037Z",
     "iopub.status.idle": "2024-09-12T20:11:43.806629Z",
     "shell.execute_reply": "2024-09-12T20:11:43.805768Z",
     "shell.execute_reply.started": "2024-09-12T20:11:43.751162Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# questions for retrieval\n",
    "questions = {\n",
    "    \"question\": [\n",
    "        \"Что дает программа \"\"Развитие материально-технической базы\"\" для детей с ограниченными возможностями при росте бюджета на 20%??\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "question_list = []\n",
    "for texts in questions[\"question\"]:\n",
    "    dataset_text = ''.join([char.lower() if not char.isdigit() and char is not None else char for char in texts])\n",
    "    dataset_text = re.sub('  ', ' ', dataset_text) # remove useless space\n",
    "    dataset_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]+', '', dataset_text)\n",
    "    question_list.append(dataset_text)\n",
    "\n",
    "questions[\"question\"] = question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ad269-75cc-426e-9951-cfaf5a0e2118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:43.808521Z",
     "iopub.status.busy": "2024-09-12T20:11:43.807648Z",
     "iopub.status.idle": "2024-09-12T20:11:45.320360Z",
     "shell.execute_reply": "2024-09-12T20:11:45.319625Z",
     "shell.execute_reply.started": "2024-09-12T20:11:43.808489Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieval Grader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatYandexGPT\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "# LLM with function call\n",
    "llm = ChatYandexGPT(iam_token=iam_token, folder_id=folder_id, temperature=0, sleep_interval=0.1)\n",
    "# structured_llm_grader = llm.with_structured_output(dict_schema)\n",
    "\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Ты - грейдер, оценивающий релевантность найденных фрагментов документов вопросу пользователя. \\n \n",
    "    Это не обязательно должны быть строгие текста. Цель - отсеять ошибочные запросы. \\n\n",
    "    Если фрагмент содержит слова или семантические значения, связанные с вопросом пользователя, оцени его как релевантный. \\n\n",
    "    Дай бинарную оценку «да» или «нет», чтобы указать, релевантен ли фрагмент вопросу. \\n\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Документы, которые мы извлекли: \\n\\n {document} \\n\\n Вопрос пользователя: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | llm\n",
    "docs = retriever.invoke(questions[\"question\"][-1])\n",
    "doc_txts = docs[-1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": questions[\"question\"][-1], \"document\": doc_txts}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1c431-5d7c-49fe-97f9-38549e1b9882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:45.322788Z",
     "iopub.status.busy": "2024-09-12T20:11:45.321255Z",
     "iopub.status.idle": "2024-09-12T20:11:45.341612Z",
     "shell.execute_reply": "2024-09-12T20:11:45.340590Z",
     "shell.execute_reply.started": "2024-09-12T20:11:45.322751Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96a00e-3cdd-4dfa-a42f-df3ce13a1504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:45.343318Z",
     "iopub.status.busy": "2024-09-12T20:11:45.342633Z",
     "iopub.status.idle": "2024-09-12T20:11:49.329468Z",
     "shell.execute_reply": "2024-09-12T20:11:49.328691Z",
     "shell.execute_reply.started": "2024-09-12T20:11:45.343282Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatYandexGPT(iam_token=iam_token, folder_id=folder_id, temperature=0, sleep_interval=0.1)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": questions[\"question\"][-1]})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf738a-7601-4e2b-9a9e-b55f43aab12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:49.331099Z",
     "iopub.status.busy": "2024-09-12T20:11:49.330591Z",
     "iopub.status.idle": "2024-09-12T20:11:51.086351Z",
     "shell.execute_reply": "2024-09-12T20:11:51.085644Z",
     "shell.execute_reply.started": "2024-09-12T20:11:49.331066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatYandexGPT(iam_token=iam_token, folder_id=folder_id, temperature=0, sleep_interval=0.1)\n",
    "# structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Ты занимаешься оцениванием, насколько LLM-вывод обоснован / подкреплен набором полученных фактов. \\n \n",
    "     В качестве ответа ты выдаешь бинарную оценку «да» или «нет». Да» означает, что ответ обоснован / подкреплен набором фактов, а имена собственные совпадают / не перепутаны с изначально упомянутыми в вопросе.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Набор фактов: \\n\\n {documents} \\n\\n Ответ LLM: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | llm\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6030796-79d3-4f2c-9fef-c3aba320b472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:51.088041Z",
     "iopub.status.busy": "2024-09-12T20:11:51.087519Z",
     "iopub.status.idle": "2024-09-12T20:11:51.650454Z",
     "shell.execute_reply": "2024-09-12T20:11:51.649783Z",
     "shell.execute_reply.started": "2024-09-12T20:11:51.088010Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Бинарный балл для оценки ответа на вопрос.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Ответьте на вопрос: «да» или «нет».\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatYandexGPT(iam_token=iam_token, folder_id=folder_id, temperature=0, sleep_interval=0.1)\n",
    "# structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Ты оцениваешь, насколько ответ LLM соответствует/решает вопрос \\n \n",
    "     Дай бинарную оценку «да» или «нет». Да» означает, что ответ разрешает вопрос. Спасибо!\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Вопрос пользователя: \\n\\n {question} \\n\\n Ответ LLM: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | llm\n",
    "answer_grader.invoke({\"question\": questions[\"question\"][-1], \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce05829-db7a-4be4-8b43-5360f6f50ac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:51.651945Z",
     "iopub.status.busy": "2024-09-12T20:11:51.651458Z",
     "iopub.status.idle": "2024-09-12T20:11:53.593810Z",
     "shell.execute_reply": "2024-09-12T20:11:53.593116Z",
     "shell.execute_reply.started": "2024-09-12T20:11:51.651914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatYandexGPT(iam_token=iam_token, folder_id=folder_id, temperature=0, sleep_interval=0.1)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"Ты преобразуешь вопрос в лучшую версию, оптимизированную \\n \n",
    "     для поиска в векторном хранилище. Посмотри на входные данные и попытайся определить семантическое намерение / смысл.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Вот начальный вопрос: \\n\\n {question} \\n Сформулируй улучшенный вопрос.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": questions[\"question\"][-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44164532-a0c7-4bed-821f-df6fe0d1655c",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "теперь объеденим описанные этапы в единый пайплайн для развертывания на проде и удобного оценивания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf08b40b-2a06-4b4b-9493-cb650d4c2fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.595504Z",
     "iopub.status.busy": "2024-09-12T20:11:53.594927Z",
     "iopub.status.idle": "2024-09-12T20:11:53.608318Z",
     "shell.execute_reply": "2024-09-12T20:11:53.607648Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.595471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "981a8e63-72a1-48a0-a884-aa17ba4f5cb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.609816Z",
     "iopub.status.busy": "2024-09-12T20:11:53.609240Z",
     "iopub.status.idle": "2024-09-12T20:11:53.621338Z",
     "shell.execute_reply": "2024-09-12T20:11:53.620599Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.609783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.get_relevant_documents(question) # our database used here\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6cd094-666c-4ed9-8a8a-1675a7132bb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.623039Z",
     "iopub.status.busy": "2024-09-12T20:11:53.622280Z",
     "iopub.status.idle": "2024-09-12T20:11:53.642099Z",
     "shell.execute_reply": "2024-09-12T20:11:53.641434Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.623005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5c0454-e76c-480a-bfff-dc111ed38e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.643700Z",
     "iopub.status.busy": "2024-09-12T20:11:53.642935Z",
     "iopub.status.idle": "2024-09-12T20:11:53.656107Z",
     "shell.execute_reply": "2024-09-12T20:11:53.655179Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.643664Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"document\": d.page_content, \"question\": question}\n",
    "        )\n",
    "        grade = score.content.lower()\n",
    "        if \"да\" in grade:\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        time.sleep(1)\n",
    "    return {\"documents\": filtered_docs, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0883a722-4598-47cb-88d8-2bc45676957a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.658572Z",
     "iopub.status.busy": "2024-09-12T20:11:53.657221Z",
     "iopub.status.idle": "2024-09-12T20:11:53.669932Z",
     "shell.execute_reply": "2024-09-12T20:11:53.668671Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.658539Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(f\"--- REWRITED QUESTION: {better_question} ---\")\n",
    "    return {\"documents\": documents, \"question\": better_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26ee3477-692f-4dca-a551-cc7eaffbb951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.671111Z",
     "iopub.status.busy": "2024-09-12T20:11:53.670705Z",
     "iopub.status.idle": "2024-09-12T20:11:53.685673Z",
     "shell.execute_reply": "2024-09-12T20:11:53.684683Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.671080Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbe79017-a0ac-42e1-9327-1fd8278aab11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.687897Z",
     "iopub.status.busy": "2024-09-12T20:11:53.686687Z",
     "iopub.status.idle": "2024-09-12T20:11:53.718876Z",
     "shell.execute_reply": "2024-09-12T20:11:53.718184Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.687862Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.content.lower()\n",
    "\n",
    "    # Check hallucination\n",
    "    if \"да\" in grade:\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.content.lower()\n",
    "        if \"да\" in grade:\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e086c4c-4705-472a-9b98-e64ddb647f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.721037Z",
     "iopub.status.busy": "2024-09-12T20:11:53.719865Z",
     "iopub.status.idle": "2024-09-12T20:11:53.826787Z",
     "shell.execute_reply": "2024-09-12T20:11:53.826018Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.721000Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429aa719-8239-42a1-bd29-9a41720dff34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.829312Z",
     "iopub.status.busy": "2024-09-12T20:11:53.828549Z",
     "iopub.status.idle": "2024-09-12T20:11:53.855357Z",
     "shell.execute_reply": "2024-09-12T20:11:53.854656Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.829275Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# questions for retrieval\n",
    "questions = {\n",
    "    \"question\": \"Что делать муниципалитету, если региональный оператор не работает?\"\n",
    "}\n",
    "\n",
    "dataset_text = ''.join([char.lower() if not char.isdigit() and char is not None else char for char in questions[\"question\"]])\n",
    "dataset_text = re.sub('  ', ' ', dataset_text) # remove useless space\n",
    "dataset_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]+', '', dataset_text)\n",
    "\n",
    "questions[\"question\"] = dataset_text\n",
    "dataset_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239dc2e-b92f-4144-b942-ed50abd24841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:11:53.858442Z",
     "iopub.status.busy": "2024-09-12T20:11:53.857747Z",
     "iopub.status.idle": "2024-09-12T20:12:28.456375Z",
     "shell.execute_reply": "2024-09-12T20:12:28.455726Z",
     "shell.execute_reply.started": "2024-09-12T20:11:53.858404Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "for output in app.stream(questions):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170b2f8-c828-49db-ad73-582b7ac8333f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
