{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad855fb-4fb3-4773-9f69-dfc01844991d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:31.537740Z",
     "iopub.status.busy": "2024-09-13T06:40:31.537160Z",
     "iopub.status.idle": "2024-09-13T06:40:47.827764Z",
     "shell.execute_reply": "2024-09-13T06:40:47.826877Z",
     "shell.execute_reply.started": "2024-09-13T06:40:31.537704Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_community in /home/jupyter/.local/lib/python3.10/site-packages (0.2.16)\n",
      "Requirement already satisfied: tiktoken in /home/jupyter/.local/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain-openai in /home/jupyter/.local/lib/python3.10/site-packages (0.1.24)\n",
      "Requirement already satisfied: langchainhub in /home/jupyter/.local/lib/python3.10/site-packages (0.1.21)\n",
      "Requirement already satisfied: langchain in /home/jupyter/.local/lib/python3.10/site-packages (0.2.16)\n",
      "Requirement already satisfied: langgraph in /home/jupyter/.local/lib/python3.10/site-packages (0.2.19)\n",
      "Requirement already satisfied: langchain_huggingface in /home/jupyter/.local/lib/python3.10/site-packages (0.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_community) (0.5.14)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_community) (0.2.39)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_community) (0.1.116)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-openai) (1.44.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/jupyter/.local/lib/python3.10/site-packages (from langchainhub) (23.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /home/jupyter/.local/lib/python3.10/site-packages (from langchainhub) (2.32.0.20240907)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain) (2.6.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in /home/jupyter/.local/lib/python3.10/site-packages (from langgraph) (1.0.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_huggingface) (0.24.7)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_huggingface) (3.1.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain_huggingface) (4.44.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/jupyter/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2023.7.22)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.0.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.10.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (9.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (2.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.1.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jupyter/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/jupyter/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/jupyter/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (16.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymilvus in /home/jupyter/.local/lib/python3.10/site-packages (2.4.6)\n",
      "Requirement already satisfied: sentence-transformers in /home/jupyter/.local/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: fsspec in /home/jupyter/.local/lib/python3.10/site-packages (2024.6.1)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: s3fs in /home/jupyter/.local/lib/python3.10/site-packages (2024.9.0)\n",
      "Requirement already satisfied: yandexcloud in /home/jupyter/.local/lib/python3.10/site-packages (0.316.0)\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (4.44.2)\n",
      "Requirement already satisfied: pip in /kernel/lib/python3.10/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement install (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for install\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface-hub in /home/jupyter/.local/lib/python3.10/site-packages (0.24.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests->huggingface-hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\n",
      "spacy 3.5.4 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.6.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.1.10 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydantic==2.6.0 in /home/jupyter/.local/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic==2.6.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic==2.6.0) (2.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic==2.6.0) (4.12.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain_community tiktoken langchain-openai langchainhub langchain langgraph langchain_huggingface\n",
    "%pip install -U pymilvus sentence-transformers fsspec s3fs yandexcloud transformers pip install langchain-experimental\n",
    "%pip install --upgrade huggingface-hub\n",
    "%pip install -qU  langchain_milvus\n",
    "%pip install pydantic==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ef898-0859-4488-aee1-00c4a90cd720",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Yandex Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5afa18-330a-413a-a10b-c8c62aa0b785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:54.333911Z",
     "iopub.status.busy": "2024-09-13T06:40:54.332906Z",
     "iopub.status.idle": "2024-09-13T06:40:54.352325Z",
     "shell.execute_reply": "2024-09-13T06:40:54.351594Z",
     "shell.execute_reply.started": "2024-09-13T06:40:54.333871Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set keys here\n",
    "service_account_id = \"ajelb1fgokcofo2dgcq2\"\n",
    "key_id = \"ajeinv58ca4aeosoqncn\"\n",
    "private_key = \"\"\"\n",
    "PLEASE DO NOT REMOVE THIS LINE! Yandex.Cloud SA Key ID <ajeinv58ca4aeosoqncn>\n",
    "-----BEGIN PRIVATE KEY-----\n",
    "MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDdMSrElRTJ99DW\n",
    "M0p0jeI55UgagNvuuj56CbBKvBLc8TFlBy82Awarg5p28Cyp3sYBaHFFOHSm73+j\n",
    "WwK5m6WXTYy0Az83Vz+Km07ONUifCgf6WmMgoErBtjfSTPQirWnyq19veiLul7xg\n",
    "7Tf14gcLqfEosZmIZOD/gxxGgCxb+2NFKJFCKvUFc9Z3hweksuacwtafqmhYNVtF\n",
    "adlPIzwhZ7G7+bPzDW16VjyG5CMlPksZms7S/7X/rc+EtGgnT2z2EnwqbEWsoT8w\n",
    "AERHLkvGPMjxF8nSvQ8mf1lNIIOrDHrJrV9/DRyKfs3d6HO0Pa0WT1PrrwBlwpnV\n",
    "bVhsMHSZAgMBAAECggEASZjIcCxihGkOdZcPWQS0lyrw+NCTXTVfAGAk5lj5tcYS\n",
    "91iSntgW6g6Z4KU9VzAmleVYev2z4q+huorXt0ZQrK1C+cpyyEkhfU77w6T7Ct/d\n",
    "k7FrdjmfZiDpJfIw8TDeJb5bvM3YvaaDKYUEr62LRpg/A5lESpu2OY44ZETVfazj\n",
    "AU8M6VMzQ7mqjkjuGIMpuF9lyys0H8ju8/6le2hVdbe9wP9p9iWg9J+9iB0w1Ke0\n",
    "rWaboQpwRMOAw3fX4WBwBZkF5hgLxsADnYsuVaSdUPFLnISKiv+kYG1GTSqx+2zo\n",
    "9puey38F6nRDuDtFfJO3S6+qFFLwaQxIaPpJw7unWQKBgQDlx2B4keMqAlbx2nXn\n",
    "KuG2M1iN6/iXT8qmNXK8GNNMTxExUj39oSYt0mBpC7NwBwgQZrIOhFdm2uCsLQDt\n",
    "m4uA7OsLj2kFBv2WaOgH0XuVAdWQCyoy5fQ9Mw2Kt3geH0h+TdenIiwETH6u/7wt\n",
    "/vCUIv9sP4KQtENwZWNRkY1upwKBgQD2bvEW7FckUYA7j0pRsi1kNq+MIRM+D7Tl\n",
    "C0wp/Amgz8O2Vl9I1sG3DDv1Vzv1eUKWZdthDc42r+vy9nzZUco+wS0gvNkDkRjR\n",
    "r0beUdVV2Q7oh3GUlRXXU7c6Zgdz6gbCGF4fBtn8uuMk/YG98y4DAP1UUEkGqyAl\n",
    "RFXJjFCqvwKBgQC0y5GejLt+3GRo3AmVIGqEoBX3ZUouVHwRF1D1q1rmWfgfJKTe\n",
    "IaQWvcdaH/jKFt5DeWp0fbD/nwzUrHxkeNTlVoUCjY6GhB+X72dSb4OblNvjAMXt\n",
    "Un5AgSEQmpeKq/awWrNqMDsODtpG+7WnQ0csZ/UtyTMEhLHjiAMDtVDhVQKBgAJD\n",
    "fp9xSFOjFAR4Cny5oEUdY3tsCls1lbnM4sQ39natseSI4pMutdTSnfJg/MICfSQY\n",
    "h21azRwffZFbxkXQxITTDXERiwTHXmz+qS39nnINbl+gbuCohezWbgZxTXw5GBrM\n",
    "UoECdRonNVLvqTNvemq4pZsSqbkP9VmiSQ9y5ILNAoGBAKuFhBcSU66uvrJIYgrs\n",
    "TO3q4+lbIE4D3kFsZ40OGtttIqfKF8GCXzxNSrU1cSZmMX0V+nA6sCHh+UF/rYMw\n",
    "hQ1XjhP3+szls8AKvQa0rKKnDHWwdjv0FXxeD8jZBVY8ekQbg7ECHQLkGWuUnhpM\n",
    "SeHk81fwBqye8o0CyyKoFsph\n",
    "-----END PRIVATE KEY-----\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e13716-438c-408b-b13e-11c3f9c6086e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:55.624850Z",
     "iopub.status.busy": "2024-09-13T06:40:55.623904Z",
     "iopub.status.idle": "2024-09-13T06:40:56.174707Z",
     "shell.execute_reply": "2024-09-13T06:40:56.173873Z",
     "shell.execute_reply.started": "2024-09-13T06:40:55.624815Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhbGciOiJQUzI1NiIsImtpZCI6ImFqZWludjU4Y2E0YWVvc29xbmNuIiwidHlwIjoiSldUIn0.eyJhdWQiOiJodHRwczovL2lhbS5hcGkuY2xvdWQueWFuZGV4Lm5ldC9pYW0vdjEvdG9rZW5zIiwiaXNzIjoiYWplbGIxZmdva2NvZm8yZGdjcTIiLCJpYXQiOjE3MjYyMDk2NTYsImV4cCI6MTcyNjIxMzI1Nn0.vB3StGap0UoA1AtwwYQV9zL-G-XY5Cx22L9v-RwfUUUjuUl7BBVsZsPUeJD7B6gheY8eWTiZYITuu1NBwoQ7uBzPjKekDrBFpzo8YpjcB1Ugn07Ab8EUMrepTbifibJFmaP86DxchGkbUY39LY54XusqcwATLTLUUC8YnqTGVhyUoRYrt5N-i65uaWt8GF0R3-1Mo1doFJaEqMkIHQKpPs6nXpqLFSSp7dToWF1vHjZaK38nxXLlQrDOc81cEFBkkhZUOwo0E090sxeYEwUsngPNwtRjKoK6S1xxQi6Oi42bjdQQiEzs929N3TsLMldUmJcClbLjucKAgOjNQlLMAQ\n"
     ]
    }
   ],
   "source": [
    "# prepare yandex gpt model for connections\n",
    "import time\n",
    "\n",
    "import jwt\n",
    "import requests\n",
    "\n",
    "# Получаем IAM-токен\n",
    "\n",
    "now = int(time.time())\n",
    "payload = {\n",
    "    \"aud\": \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "    \"iss\": service_account_id,\n",
    "    \"iat\": now,\n",
    "    \"exp\": now + 3600,\n",
    "}\n",
    "\n",
    "\n",
    "# Формирование JWT.\n",
    "encoded_token = jwt.encode(\n",
    "    payload, private_key, algorithm=\"PS256\", headers={\"kid\": key_id}\n",
    ")\n",
    "\n",
    "\n",
    "# Запись ключа в файл\n",
    "with open(\"jwt_token.txt\", \"w\") as j:\n",
    "    j.write(encoded_token)\n",
    "\n",
    "# Вывод в консоль\n",
    "print(encoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b815f9-472f-482e-8a70-cf50470010b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:56.260674Z",
     "iopub.status.busy": "2024-09-13T06:40:56.259449Z",
     "iopub.status.idle": "2024-09-13T06:40:56.339678Z",
     "shell.execute_reply": "2024-09-13T06:40:56.338946Z",
     "shell.execute_reply.started": "2024-09-13T06:40:56.260630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM Token: t1.9euelZqVmp6SzZPLzYvLnJWejc6Zz-3rnpWak53OmZiQlJyQmZDNm5icjs3l8_cHQ3BI-e8EZyxl_t3z90dxbUj57wRnLGX-zef1656Vmo-UmJqSx5OSyJDHjceKkJbO7_zN5_XrnpWalpGJysecnsuempCMkI6RnJHv_cXrnpWaj5SYmpLHk5LIkMeNx4qQls4.j9EvZBcH431Gqgmbei5mBoTe7oNx_1aFMQglnUe2Q6nKpISJ746N17ivibdYvrLLbkSogjPGl1eYgpZjaFNVAw\n"
     ]
    }
   ],
   "source": [
    "def get_iam_token():\n",
    "    now = int(time.time())\n",
    "    payload = {\n",
    "        \"aud\": \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "        \"iss\": service_account_id,\n",
    "        \"iat\": now,\n",
    "        \"exp\": now + 3600,\n",
    "    }\n",
    "    encoded_token = jwt.encode(\n",
    "        payload, private_key, algorithm=\"PS256\", headers={\"kid\": key_id}\n",
    "    )\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json={\"jwt\": encoded_token},\n",
    "    )\n",
    "\n",
    "    return response.json()[\"iamToken\"]\n",
    "\n",
    "\n",
    "# Получаем IAM-токен\n",
    "iam_token = get_iam_token()\n",
    "\n",
    "# Печатаем IAM-токен для отладки\n",
    "print(\"IAM Token:\", iam_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161202f6-7ca8-4980-8299-165bc3ede9e4",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0404ba60-a38c-4db9-b505-9c3f20c5ac94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:57.532280Z",
     "iopub.status.busy": "2024-09-13T06:40:57.531383Z",
     "iopub.status.idle": "2024-09-13T06:40:57.543604Z",
     "shell.execute_reply": "2024-09-13T06:40:57.542879Z",
     "shell.execute_reply.started": "2024-09-13T06:40:57.532250Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_id = 'b1g5pkij4170jm1a1un6'\n",
    "filename = \"npa_dataset_v3.db\"  # change dataset version when new iteration begins!\n",
    "database_path = f\"/home/jupyter/datasphere/s3/hack-object-storage/database/{filename}\"\n",
    "dataset_path = (\n",
    "    \"/home/jupyter/datasphere/datasets/npa_two/preprocessed_dataset_ver_2.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e89fa-8b97-4128-8150-161c313c6045",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6789a784-d158-4bfe-b20e-ce42503aead9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:28:50.609534Z",
     "iopub.status.busy": "2024-09-12T20:28:50.607430Z",
     "iopub.status.idle": "2024-09-12T20:28:50.622608Z",
     "shell.execute_reply": "2024-09-12T20:28:50.621928Z",
     "shell.execute_reply.started": "2024-09-12T20:28:50.609494Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(dataset_path, encoding=\"utf-8\") as file:\n",
    "#     dataset_text = file.read()\n",
    "\n",
    "# data_raw = dataset_text.split(\"</s>\\n\")\n",
    "# data_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c9a64b-a90a-46f8-9083-ec211853ebd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:28:50.624584Z",
     "iopub.status.busy": "2024-09-12T20:28:50.623525Z",
     "iopub.status.idle": "2024-09-12T20:28:50.646303Z",
     "shell.execute_reply": "2024-09-12T20:28:50.645622Z",
     "shell.execute_reply.started": "2024-09-12T20:28:50.624549Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# documents = []\n",
    "# for texts in data_raw:\n",
    "#     doc = Document(page_content=texts, metadata={\"source\": dataset_path, \"database_path\": database_path})\n",
    "#     documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391146b4-87be-40f0-beb1-4606d04509e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:28:50.648422Z",
     "iopub.status.busy": "2024-09-12T20:28:50.647159Z",
     "iopub.status.idle": "2024-09-12T20:28:50.656920Z",
     "shell.execute_reply": "2024-09-12T20:28:50.656201Z",
     "shell.execute_reply.started": "2024-09-12T20:28:50.648385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_milvus import Milvus\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from transformers import AutoModel\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"deepvk/USER-bge-m3\", trust_remote_code=True) \n",
    "\n",
    "# model_name = \"deepvk/USER-bge-m3\"\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs=model_kwargs,\n",
    "# )\n",
    "\n",
    "# # Retriever\n",
    "# # API Here: https://python.langchain.com/v0.2/api_reference/milvus/vectorstores/langchain_milvus.vectorstores.milvus.Milvus.html\n",
    "# vectorstore = Milvus.from_documents(\n",
    "#     documents = documents[:100],\n",
    "#     collection_name = \"npa_storage_512_64\", \n",
    "#     connection_args={\"uri\": database_path},\n",
    "#     embedding = embeddings\n",
    "# )\n",
    "\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1aab5-74ca-4f7e-8881-1151bfbdabcf",
   "metadata": {},
   "source": [
    "## Prepare Self-Reflective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dced5f23-4e7a-444e-b22b-e6260c29dea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:43:13.903276Z",
     "iopub.status.busy": "2024-09-13T06:43:13.902130Z",
     "iopub.status.idle": "2024-09-13T06:43:13.953750Z",
     "shell.execute_reply": "2024-09-13T06:43:13.952948Z",
     "shell.execute_reply.started": "2024-09-13T06:43:13.903242Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatYandexGPT\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "import time\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fc91aa9-4592-47fe-994f-16898ceef5e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T07:12:15.405070Z",
     "iopub.status.busy": "2024-09-13T07:12:15.404016Z",
     "iopub.status.idle": "2024-09-13T07:12:15.458860Z",
     "shell.execute_reply": "2024-09-13T07:12:15.458076Z",
     "shell.execute_reply.started": "2024-09-13T07:12:15.405021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "class SelfReflectiveRag:\n",
    "    \n",
    "    def __init__(self, db_path : str, db_name : str, gpt_folder_id : str, iam_token : str):\n",
    "        self.db_path = db_path\n",
    "        self.db_name = db_name\n",
    "        self.gpt_folder_id = gpt_folder_id\n",
    "        self.iam_token = iam_token\n",
    "        self.vectorstore = None\n",
    "        self.embeddings = None\n",
    "        self.retriever = None\n",
    "        self.rag_chain = None\n",
    "        self.retrieval_grader = None\n",
    "        self.hallucination_grader = None\n",
    "        self.answer_grader = None\n",
    "        self.question_rewriter = None\n",
    "        self.app = None\n",
    "\n",
    "    def run_rag(self, questions):\n",
    "        self._compile_app()\n",
    "        self._preprocess_question(questions)\n",
    "        question = {\"question\" : self.questions[\"questions\"][-1]}\n",
    "\n",
    "        # Run\n",
    "        for output in self.app.stream(question):\n",
    "            for key, value in output.items():\n",
    "                # Node\n",
    "                pprint(f\"Node '{key}':\")\n",
    "                # Optional: print full state at each node\n",
    "                # pprint.pprint(value[\"documents\"], indent=2, width=80, depth=None)\n",
    "            pprint(\"\\n---\\n\")\n",
    "        # Final generation\n",
    "        pprint(value[\"generation\"])\n",
    "        print(value)\n",
    "        return value\n",
    "    \n",
    "    \n",
    "    def _compile_app(self):\n",
    "        workflow = StateGraph(GraphState)\n",
    "        \n",
    "        self._get_retriever()\n",
    "        self._get_retrieval_grader()\n",
    "        self._get_generator_query()\n",
    "        self._get_hallucination_grader()\n",
    "        self._get_answer_grader()\n",
    "        self._get_question_writer()\n",
    "\n",
    "        # Define the nodes\n",
    "        workflow.add_node(\"retrieve\", self._retrieve)  # retrieve\n",
    "        workflow.add_node(\"grade_documents\", self._grade_documents)  # grade documents\n",
    "        workflow.add_node(\"generate\", self._generate)  # generate\n",
    "        workflow.add_node(\"transform_query\", self._transform_query)  # transform query\n",
    "\n",
    "        # Build graph\n",
    "        workflow.add_edge(START, \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"grade_documents\",\n",
    "            self._decide_to_generate,\n",
    "            {\n",
    "                \"transform_query\": \"transform_query\",\n",
    "                \"generate\": \"generate\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"generate\",\n",
    "            self._grade_generation_v_documents_and_question,\n",
    "            {\n",
    "                \"not supported\": \"transform_query\",\n",
    "                \"useful\": END,\n",
    "                \"not useful\": \"transform_query\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Compile\n",
    "        self.app = workflow.compile()\n",
    "\n",
    "        \n",
    "    def _preprocess_question(self, questions: List[str]):\n",
    "        # questions for retrieval\n",
    "        self.questions = {\n",
    "            \"questions_raw\": questions\n",
    "        }\n",
    "        question_list = []\n",
    "        for texts in questions:\n",
    "            dataset_text = ''.join([char.lower() if not char.isdigit() and char is not None else char for char in texts])\n",
    "            dataset_text = re.sub('  ', ' ', dataset_text)  # remove useless space\n",
    "            dataset_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]+', '', dataset_text)\n",
    "            question_list.append(dataset_text)\n",
    "\n",
    "        self.questions[\"questions\"] = question_list\n",
    "        \n",
    "        \n",
    "    def _get_retriever(self, model_name=\"deepvk/USER-bge-m3\", collection_name=\"npa_storage_512_64\"):\n",
    "        # Retriever\n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model_kwargs = {'device': 'cpu'}\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "\n",
    "        self.vectorstore = Milvus(\n",
    "            collection_name=collection_name, \n",
    "            connection_args={\"uri\": self.db_path},\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 20, \"fetch_k\": 50, \"lambda_mult\": 0.8})\n",
    "\n",
    "    \n",
    "    def _get_retrieval_grader(self):\n",
    "        # Retrieval Grader\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "        \n",
    "        # Prompt\n",
    "        system = \"\"\"Ты - грейдер, оценивающий релевантность найденных фрагментов документов вопросу пользователя. \\n \n",
    "            Это не обязательно должны быть строгие текста. Цель - отсеять ошибочные запросы. \\n\n",
    "            Если фрагмент содержит слова или семантические значения, связанные с вопросом пользователя, оцени его как релевантный. \\n\n",
    "            Дай бинарную оценку «да» или «нет», чтобы указать, релевантен ли фрагмент вопросу. \\n\"\"\"\n",
    "\n",
    "        grade_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", \"Документы, которые мы извлекли: \\n\\n {document} \\n\\n Вопрос пользователя: {question}\"),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.retrieval_grader = grade_prompt | llm\n",
    "        \n",
    "    \n",
    "    def _get_generator_query(self):\n",
    "        ### Generate\n",
    "        # Prompt\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        # LLM\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "        # Chain\n",
    "        self.rag_chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "    \n",
    "    def _get_hallucination_grader(self):\n",
    "        ### Hallucination Grader\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "\n",
    "        # Prompt\n",
    "        system = \"\"\"Ты занимаешься оцениванием, насколько LLM-вывод обоснован / подкреплен набором полученных фактов. \\n \n",
    "             В качестве ответа ты выдаешь бинарную оценку «да» или «нет». Да» означает, что ответ обоснован / подкреплен набором фактов, а имена собственные совпадают / не перепутаны с изначально упомянутыми в вопросе.\"\"\"\n",
    "        hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", \"Набор фактов: \\n\\n {documents} \\n\\n Ответ LLM: {generation}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.hallucination_grader = hallucination_prompt | llm\n",
    "    \n",
    "    \n",
    "    def _get_answer_grader(self):\n",
    "        ### Answer Grader\n",
    "        # LLM with function call\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "        # Prompt\n",
    "        system = \"\"\"Ты оцениваешь, насколько ответ LLM соответствует/решает вопрос \\n \n",
    "             Дай бинарную оценку «да» или «нет». Да» означает, что ответ разрешает вопрос. Спасибо!\"\"\"\n",
    "        \n",
    "        answer_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", \"Вопрос пользователя: \\n\\n {question} \\n\\n Ответ LLM: {generation}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.answer_grader = answer_prompt | llm\n",
    "\n",
    "        \n",
    "    def _get_question_writer(self):\n",
    "        ### Question Re-writer\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "\n",
    "        # Prompt\n",
    "        system = \"\"\"Ты преобразуешь вопрос в лучшую версию, оптимизированную \\n \n",
    "             для поиска в векторном хранилище. Посмотри на входные данные и попытайся определить семантическое намерение / смысл.\"\"\"\n",
    "        re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Вот начальный вопрос: \\n\\n {question} \\n Сформулируй улучшенный вопрос.\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    \n",
    "    ### Nodes\n",
    "    def _retrieve(self, state):\n",
    "        \"\"\"\n",
    "        Retrieve documents\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): New key added to state, documents, that contains retrieved documents\n",
    "        \"\"\"\n",
    "        print(\"---RETRIEVE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        # Retrieval\n",
    "        documents = self.retriever.get_relevant_documents(question) # our database used here\n",
    "        return {\"documents\": documents, \"question\": question}\n",
    "    \n",
    "    \n",
    "    def _generate(self, state):\n",
    "        \"\"\"\n",
    "        Generate LLM response based on query + documents.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation, that contains final generation\n",
    "        \"\"\"\n",
    "        print(\"---GENERATE---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # RAG generation\n",
    "        generation = self.rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "        time.sleep(1)\n",
    "        return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    \n",
    "    \n",
    "    def _grade_documents(self, state):\n",
    "        \"\"\"\n",
    "        Score retrieved documents\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): Existing key in state, documents, that now has graded documents\n",
    "        \"\"\"\n",
    "        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # Score each doc\n",
    "        filtered_docs = []\n",
    "        for d in documents:\n",
    "            score = self.retrieval_grader.invoke(\n",
    "                {\"document\": d.page_content, \"question\": question}\n",
    "            )\n",
    "            time.sleep(1)\n",
    "            grade = score.content.lower()\n",
    "            if \"да\" in grade:\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        return {\"documents\": filtered_docs, \"question\": question}\n",
    "    \n",
    "    \n",
    "    def _grade_generation_v_documents_and_question(self, state):\n",
    "        \"\"\"\n",
    "        Score generation\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): decision: if useful or not useful\n",
    "        \"\"\"\n",
    "        print(\"---CHECK HALLUCINATIONS---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "        generation = state[\"generation\"]\n",
    "\n",
    "        score = self.hallucination_grader.invoke(\n",
    "            {\"documents\": documents, \"generation\": generation}\n",
    "        )\n",
    "        time.sleep(1)\n",
    "        grade = score.content.lower()\n",
    "\n",
    "        # Check hallucination\n",
    "        if \"да\" in grade:\n",
    "            print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "            # Check question-answering\n",
    "            print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "            score = self.answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "            time.sleep(1)\n",
    "            grade = score.content.lower()\n",
    "            if \"да\" in grade:\n",
    "                print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "                return \"useful\"\n",
    "            else:\n",
    "                print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "                return \"not useful\"\n",
    "        else:\n",
    "            pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "            return \"not supported\"\n",
    "    \n",
    "    \n",
    "    def _transform_query(self, state):\n",
    "        \"\"\"\n",
    "        Re-write query.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): New key in state, question, that now has the transformed query\n",
    "        \"\"\"\n",
    "        print(\"---TRANSFORM QUERY---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # Re-write question\n",
    "        better_question = self.question_rewriter.invoke({\"question\": question})\n",
    "        time.sleep(1)\n",
    "        print(f\"--- REWRITED QUESTION: {better_question} ---\")\n",
    "        return {\"documents\": documents, \"question\": better_question}\n",
    "    \n",
    "    \n",
    "    def _decide_to_generate(self, state):\n",
    "        \"\"\"\n",
    "        Decide if query should be transformed or generate LLM response.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            str: Edge to follow, \"generate\" or \"transform_query\"\n",
    "        \"\"\"\n",
    "        print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "        state[\"question\"]\n",
    "        filtered_documents = state[\"documents\"]\n",
    "\n",
    "        if not filtered_documents:\n",
    "            # All documents have been filtered check_relevance\n",
    "            # We will re-generate a new query\n",
    "            print(\n",
    "                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "            )\n",
    "            return \"transform_query\"\n",
    "        else:\n",
    "            # We have relevant documents, so generate answer\n",
    "            print(\"---DECISION: GENERATE---\")\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b853311-b16b-47c8-b5a6-7c4ece83d6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T07:16:35.119685Z",
     "iopub.status.busy": "2024-09-13T07:16:35.118685Z",
     "iopub.status.idle": "2024-09-13T07:17:28.930526Z",
     "shell.execute_reply": "2024-09-13T07:17:28.929619Z",
     "shell.execute_reply.started": "2024-09-13T07:16:35.119644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/langsmith/client.py:312: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'Учебно-наглядные пособия и оборудованные кабинеты.'\n",
      "{'question': 'чем обновлять школы округа?', 'generation': 'Учебно-наглядные пособия и оборудованные кабинеты.', 'documents': [Document(metadata={'database_path': '/home/jupyter/datasphere/s3/hack-object-storage/database/npa_dataset_v3.db', 'pk': 452495725165876300, 'source': '/home/jupyter/datasphere/datasets/npa_two/preprocessed_dataset_ver_2.txt'}, page_content='по оснащению образовательного процесса, реализованные за период 2007 - 2010 гг.: более 30% школьников автономного округа обучаются в условиях, не соответствующих современным требованиям; больше половины образовательных учреждений автономного округа нуждаются в обновлении учебно-наглядных пособий, современных оборудованных кабинетах. образовательные учреждения автономного округа обеспечены доступом к сети интернет, однако в большинстве учреждений отсутствуют локальные внутренние сети, необходимые для'), Document(metadata={'database_path': '/home/jupyter/datasphere/s3/hack-object-storage/database/npa_dataset_v3.db', 'pk': 452495711034417904, 'source': '/home/jupyter/datasphere/datasets/npa_two/preprocessed_dataset_ver_2.txt'}, page_content='для перевозки обучающихся 0,000 0,000 0,000 3 пополнение фондов библиотек общеобразовательных учреждений 80,732 80,732 0,0 4 развитие школьной инфраструктуры (текущий ремонт с целью обеспечения выполнения требований к санитарно- бытовым условиям и охране здоровья обучающихся, а также с целью подготовки помещений для установки оборудования) 0,000 0,000 0,000 5 повышение квалификации, профессиональная переподготовка руководителей общеобразовательных учреждений и учителей * 0,000 0,000 0,000 6 модернизация')]}\n"
     ]
    }
   ],
   "source": [
    "reflective_rag = SelfReflectiveRag(\n",
    "    db_path = database_path, db_name = filename, gpt_folder_id = folder_id, iam_token = iam_token\n",
    ")\n",
    "\n",
    "questions =  [\n",
    "        \"Чем обновлять школы округа?\"\n",
    "    ]\n",
    "\n",
    "value = reflective_rag.run_rag(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "530111c8-2e18-4f03-9136-65ac7f7c88ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T07:18:07.947633Z",
     "iopub.status.busy": "2024-09-13T07:18:07.946625Z",
     "iopub.status.idle": "2024-09-13T07:18:07.958503Z",
     "shell.execute_reply": "2024-09-13T07:18:07.957874Z",
     "shell.execute_reply.started": "2024-09-13T07:18:07.947580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = value['question']\n",
    "generation = value['generation']\n",
    "documents = value['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9158f8f-f04a-4541-a9a2-cc0161ba4cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:00:20.894439Z",
     "iopub.status.busy": "2024-09-13T08:00:20.893569Z",
     "iopub.status.idle": "2024-09-13T08:00:20.912215Z",
     "shell.execute_reply": "2024-09-13T08:00:20.911518Z",
     "shell.execute_reply.started": "2024-09-13T08:00:20.894412Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        page_content\n",
      "0  по оснащению образовательного процесса, реализ...\n",
      "1  для перевозки обучающихся 0,000 0,000 0,000 3 ...\n"
     ]
    }
   ],
   "source": [
    "# Извлечение содержимого page_content из объектов Document\n",
    "page_contents = [doc.page_content for doc in documents]\n",
    "\n",
    "# Создание DataFrame\n",
    "df = pd.DataFrame(page_contents, columns=['page_content'])\n",
    "\n",
    "# Вывод DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34d2aaff-08cf-4fc6-9174-a84bf2950bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:05:00.584184Z",
     "iopub.status.busy": "2024-09-13T08:05:00.583459Z",
     "iopub.status.idle": "2024-09-13T08:05:00.609852Z",
     "shell.execute_reply": "2024-09-13T08:05:00.609080Z",
     "shell.execute_reply.started": "2024-09-13T08:05:00.584159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Добавление колонок question и generation\n",
    "df['question'] = question\n",
    "df['answer'] = generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25b7518f-a277-4e0f-9ca6-9170979b910f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:06:17.800427Z",
     "iopub.status.busy": "2024-09-13T08:06:17.799401Z",
     "iopub.status.idle": "2024-09-13T08:06:17.812757Z",
     "shell.execute_reply": "2024-09-13T08:06:17.811889Z",
     "shell.execute_reply.started": "2024-09-13T08:06:17.800395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'page_content': 'contexts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a00cf248-4aa8-456a-8525-0f3f8930c332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:06:18.801876Z",
     "iopub.status.busy": "2024-09-13T08:06:18.800862Z",
     "iopub.status.idle": "2024-09-13T08:06:18.821747Z",
     "shell.execute_reply": "2024-09-13T08:06:18.821074Z",
     "shell.execute_reply.started": "2024-09-13T08:06:18.801842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contexts</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>по оснащению образовательного процесса, реализ...</td>\n",
       "      <td>чем обновлять школы округа?</td>\n",
       "      <td>Учебно-наглядные пособия и оборудованные кабин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>для перевозки обучающихся 0,000 0,000 0,000 3 ...</td>\n",
       "      <td>чем обновлять школы округа?</td>\n",
       "      <td>Учебно-наглядные пособия и оборудованные кабин...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contexts  ...                                             answer\n",
       "0  по оснащению образовательного процесса, реализ...  ...  Учебно-наглядные пособия и оборудованные кабин...\n",
       "1  для перевозки обучающихся 0,000 0,000 0,000 3 ...  ...  Учебно-наглядные пособия и оборудованные кабин...\n",
       "\n",
       "[2 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba071a1c-a704-426c-93fc-95124c4ce064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:09:15.846937Z",
     "iopub.status.busy": "2024-09-13T08:09:15.845911Z",
     "iopub.status.idle": "2024-09-13T08:09:16.833952Z",
     "shell.execute_reply": "2024-09-13T08:09:16.833094Z",
     "shell.execute_reply.started": "2024-09-13T08:09:15.846895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Сохранение DataFrame в файл Excel\n",
    "output_file = '/home/jupyter/datasphere/s3/hack-object-storage/database/file_for_merge.xlsx'\n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "595551bb-967b-4e4a-8be1-5e8bfaec5097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T22:03:28.658150Z",
     "iopub.status.busy": "2024-09-12T22:03:28.657106Z",
     "iopub.status.idle": "2024-09-12T22:03:29.952518Z",
     "shell.execute_reply": "2024-09-12T22:03:29.951459Z",
     "shell.execute_reply.started": "2024-09-12T22:03:28.658095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
