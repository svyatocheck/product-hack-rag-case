{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad855fb-4fb3-4773-9f69-dfc01844991d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:31.537740Z",
     "iopub.status.busy": "2024-09-13T06:40:31.537160Z",
     "iopub.status.idle": "2024-09-13T06:40:47.827764Z",
     "shell.execute_reply": "2024-09-13T06:40:47.826877Z",
     "shell.execute_reply.started": "2024-09-13T06:40:31.537704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain_community tiktoken langchain-openai langchainhub langchain langgraph langchain_huggingface\n",
    "%pip install -U pymilvus sentence-transformers fsspec s3fs yandexcloud transformers pip install langchain-experimental\n",
    "%pip install --upgrade huggingface-hub\n",
    "%pip install -qU  langchain_milvus\n",
    "%pip install pydantic==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ef898-0859-4488-aee1-00c4a90cd720",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Yandex Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5afa18-330a-413a-a10b-c8c62aa0b785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:54.333911Z",
     "iopub.status.busy": "2024-09-13T06:40:54.332906Z",
     "iopub.status.idle": "2024-09-13T06:40:54.352325Z",
     "shell.execute_reply": "2024-09-13T06:40:54.351594Z",
     "shell.execute_reply.started": "2024-09-13T06:40:54.333871Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set keys here\n",
    "service_account_id = \"\"\n",
    "key_id = \"\"\n",
    "private_key = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e13716-438c-408b-b13e-11c3f9c6086e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:55.624850Z",
     "iopub.status.busy": "2024-09-13T06:40:55.623904Z",
     "iopub.status.idle": "2024-09-13T06:40:56.174707Z",
     "shell.execute_reply": "2024-09-13T06:40:56.173873Z",
     "shell.execute_reply.started": "2024-09-13T06:40:55.624815Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare yandex gpt model for connections\n",
    "import time\n",
    "\n",
    "import jwt\n",
    "import requests\n",
    "\n",
    "# Получаем IAM-токен\n",
    "\n",
    "now = int(time.time())\n",
    "payload = {\n",
    "    \"aud\": \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "    \"iss\": service_account_id,\n",
    "    \"iat\": now,\n",
    "    \"exp\": now + 3600,\n",
    "}\n",
    "\n",
    "\n",
    "# Формирование JWT.\n",
    "encoded_token = jwt.encode(\n",
    "    payload, private_key, algorithm=\"PS256\", headers={\"kid\": key_id}\n",
    ")\n",
    "\n",
    "\n",
    "# Запись ключа в файл\n",
    "with open(\"jwt_token.txt\", \"w\") as j:\n",
    "    j.write(encoded_token)\n",
    "\n",
    "# Вывод в консоль\n",
    "print(encoded_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b815f9-472f-482e-8a70-cf50470010b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:56.260674Z",
     "iopub.status.busy": "2024-09-13T06:40:56.259449Z",
     "iopub.status.idle": "2024-09-13T06:40:56.339678Z",
     "shell.execute_reply": "2024-09-13T06:40:56.338946Z",
     "shell.execute_reply.started": "2024-09-13T06:40:56.260630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_iam_token():\n",
    "    now = int(time.time())\n",
    "    payload = {\n",
    "        \"aud\": \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "        \"iss\": service_account_id,\n",
    "        \"iat\": now,\n",
    "        \"exp\": now + 3600,\n",
    "    }\n",
    "    encoded_token = jwt.encode(\n",
    "        payload, private_key, algorithm=\"PS256\", headers={\"kid\": key_id}\n",
    "    )\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://iam.api.cloud.yandex.net/iam/v1/tokens\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json={\"jwt\": encoded_token},\n",
    "    )\n",
    "\n",
    "    return response.json()[\"iamToken\"]\n",
    "\n",
    "\n",
    "# Получаем IAM-токен\n",
    "iam_token = get_iam_token()\n",
    "\n",
    "# Печатаем IAM-токен для отладки\n",
    "print(\"IAM Token:\", iam_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161202f6-7ca8-4980-8299-165bc3ede9e4",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0404ba60-a38c-4db9-b505-9c3f20c5ac94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:40:57.532280Z",
     "iopub.status.busy": "2024-09-13T06:40:57.531383Z",
     "iopub.status.idle": "2024-09-13T06:40:57.543604Z",
     "shell.execute_reply": "2024-09-13T06:40:57.542879Z",
     "shell.execute_reply.started": "2024-09-13T06:40:57.532250Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_id = 'b1g5pkij4170jm1a1un6'\n",
    "filename = \"npa_dataset_v3.db\"  # change dataset version when new iteration begins!\n",
    "database_path = f\"/home/jupyter/datasphere/s3/hack-object-storage/database/{filename}\"\n",
    "dataset_path = (\n",
    "    \"/home/jupyter/datasphere/datasets/npa_two/preprocessed_dataset_ver_2.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e89fa-8b97-4128-8150-161c313c6045",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6789a784-d158-4bfe-b20e-ce42503aead9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:28:50.609534Z",
     "iopub.status.busy": "2024-09-12T20:28:50.607430Z",
     "iopub.status.idle": "2024-09-12T20:28:50.622608Z",
     "shell.execute_reply": "2024-09-12T20:28:50.621928Z",
     "shell.execute_reply.started": "2024-09-12T20:28:50.609494Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(dataset_path, encoding=\"utf-8\") as file:\n",
    "#     dataset_text = file.read()\n",
    "\n",
    "# data_raw = dataset_text.split(\"</s>\\n\")\n",
    "# data_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c9a64b-a90a-46f8-9083-ec211853ebd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:28:50.624584Z",
     "iopub.status.busy": "2024-09-12T20:28:50.623525Z",
     "iopub.status.idle": "2024-09-12T20:28:50.646303Z",
     "shell.execute_reply": "2024-09-12T20:28:50.645622Z",
     "shell.execute_reply.started": "2024-09-12T20:28:50.624549Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# documents = []\n",
    "# for texts in data_raw:\n",
    "#     doc = Document(page_content=texts, metadata={\"source\": dataset_path, \"database_path\": database_path})\n",
    "#     documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391146b4-87be-40f0-beb1-4606d04509e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T20:28:50.648422Z",
     "iopub.status.busy": "2024-09-12T20:28:50.647159Z",
     "iopub.status.idle": "2024-09-12T20:28:50.656920Z",
     "shell.execute_reply": "2024-09-12T20:28:50.656201Z",
     "shell.execute_reply.started": "2024-09-12T20:28:50.648385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_milvus import Milvus\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from transformers import AutoModel\n",
    "\n",
    "# model = AutoModel.from_pretrained(\"deepvk/USER-bge-m3\", trust_remote_code=True) \n",
    "\n",
    "# model_name = \"deepvk/USER-bge-m3\"\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs=model_kwargs,\n",
    "# )\n",
    "\n",
    "# # Retriever\n",
    "# # API Here: https://python.langchain.com/v0.2/api_reference/milvus/vectorstores/langchain_milvus.vectorstores.milvus.Milvus.html\n",
    "# vectorstore = Milvus.from_documents(\n",
    "#     documents = documents[:100],\n",
    "#     collection_name = \"npa_storage_512_64\", \n",
    "#     connection_args={\"uri\": database_path},\n",
    "#     embedding = embeddings\n",
    "# )\n",
    "\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1aab5-74ca-4f7e-8881-1151bfbdabcf",
   "metadata": {},
   "source": [
    "## Prepare Self-Reflective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dced5f23-4e7a-444e-b22b-e6260c29dea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T06:43:13.903276Z",
     "iopub.status.busy": "2024-09-13T06:43:13.902130Z",
     "iopub.status.idle": "2024-09-13T06:43:13.953750Z",
     "shell.execute_reply": "2024-09-13T06:43:13.952948Z",
     "shell.execute_reply.started": "2024-09-13T06:43:13.903242Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatYandexGPT\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "import time\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fc91aa9-4592-47fe-994f-16898ceef5e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T07:12:15.405070Z",
     "iopub.status.busy": "2024-09-13T07:12:15.404016Z",
     "iopub.status.idle": "2024-09-13T07:12:15.458860Z",
     "shell.execute_reply": "2024-09-13T07:12:15.458076Z",
     "shell.execute_reply.started": "2024-09-13T07:12:15.405021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "class SelfReflectiveRag:\n",
    "    \n",
    "    def __init__(self, db_path : str, db_name : str, gpt_folder_id : str, iam_token : str):\n",
    "        self.db_path = db_path\n",
    "        self.db_name = db_name\n",
    "        self.gpt_folder_id = gpt_folder_id\n",
    "        self.iam_token = iam_token\n",
    "        self.vectorstore = None\n",
    "        self.embeddings = None\n",
    "        self.retriever = None\n",
    "        self.rag_chain = None\n",
    "        self.retrieval_grader = None\n",
    "        self.hallucination_grader = None\n",
    "        self.answer_grader = None\n",
    "        self.question_rewriter = None\n",
    "        self.app = None\n",
    "\n",
    "    def run_rag(self, questions):\n",
    "        self._compile_app()\n",
    "        self._preprocess_question(questions)\n",
    "        question = {\"question\" : self.questions[\"questions\"][-1]}\n",
    "\n",
    "        # Run\n",
    "        for output in self.app.stream(question):\n",
    "            for key, value in output.items():\n",
    "                # Node\n",
    "                pprint(f\"Node '{key}':\")\n",
    "                # Optional: print full state at each node\n",
    "                # pprint.pprint(value[\"documents\"], indent=2, width=80, depth=None)\n",
    "            pprint(\"\\n---\\n\")\n",
    "        # Final generation\n",
    "        pprint(value[\"generation\"])\n",
    "        print(value)\n",
    "        return value\n",
    "    \n",
    "    \n",
    "    def _compile_app(self):\n",
    "        workflow = StateGraph(GraphState)\n",
    "        \n",
    "        self._get_retriever()\n",
    "        self._get_retrieval_grader()\n",
    "        self._get_generator_query()\n",
    "        self._get_hallucination_grader()\n",
    "        self._get_answer_grader()\n",
    "        self._get_question_writer()\n",
    "\n",
    "        # Define the nodes\n",
    "        workflow.add_node(\"retrieve\", self._retrieve)  # retrieve\n",
    "        workflow.add_node(\"grade_documents\", self._grade_documents)  # grade documents\n",
    "        workflow.add_node(\"generate\", self._generate)  # generate\n",
    "        workflow.add_node(\"transform_query\", self._transform_query)  # transform query\n",
    "\n",
    "        # Build graph\n",
    "        workflow.add_edge(START, \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"grade_documents\",\n",
    "            self._decide_to_generate,\n",
    "            {\n",
    "                \"transform_query\": \"transform_query\",\n",
    "                \"generate\": \"generate\",\n",
    "            },\n",
    "        )\n",
    "        workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"generate\",\n",
    "            self._grade_generation_v_documents_and_question,\n",
    "            {\n",
    "                \"not supported\": \"transform_query\",\n",
    "                \"useful\": END,\n",
    "                \"not useful\": \"transform_query\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Compile\n",
    "        self.app = workflow.compile()\n",
    "\n",
    "        \n",
    "    def _preprocess_question(self, questions: List[str]):\n",
    "        # questions for retrieval\n",
    "        self.questions = {\n",
    "            \"questions_raw\": questions\n",
    "        }\n",
    "        question_list = []\n",
    "        for texts in questions:\n",
    "            dataset_text = ''.join([char.lower() if not char.isdigit() and char is not None else char for char in texts])\n",
    "            dataset_text = re.sub('  ', ' ', dataset_text)  # remove useless space\n",
    "            dataset_text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]+', '', dataset_text)\n",
    "            question_list.append(dataset_text)\n",
    "\n",
    "        self.questions[\"questions\"] = question_list\n",
    "        \n",
    "        \n",
    "    def _get_retriever(self, model_name=\"deepvk/USER-bge-m3\", collection_name=\"npa_storage_512_64\"):\n",
    "        # Retriever\n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model_kwargs = {'device': 'cpu'}\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "\n",
    "        self.vectorstore = Milvus(\n",
    "            collection_name=collection_name, \n",
    "            connection_args={\"uri\": self.db_path},\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 20, \"fetch_k\": 50, \"lambda_mult\": 0.8})\n",
    "\n",
    "    \n",
    "    def _get_retrieval_grader(self):\n",
    "        # Retrieval Grader\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "        \n",
    "        # Prompt\n",
    "        system = \"\"\"Ты - грейдер, оценивающий релевантность найденных фрагментов документов вопросу пользователя. \\n \n",
    "            Это не обязательно должны быть строгие текста. Цель - отсеять ошибочные запросы. \\n\n",
    "            Если фрагмент содержит слова или семантические значения, связанные с вопросом пользователя, оцени его как релевантный. \\n\n",
    "            Дай бинарную оценку «да» или «нет», чтобы указать, релевантен ли фрагмент вопросу. \\n\"\"\"\n",
    "\n",
    "        grade_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", \"Документы, которые мы извлекли: \\n\\n {document} \\n\\n Вопрос пользователя: {question}\"),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.retrieval_grader = grade_prompt | llm\n",
    "        \n",
    "    \n",
    "    def _get_generator_query(self):\n",
    "        ### Generate\n",
    "        # Prompt\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "        # LLM\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "        # Chain\n",
    "        self.rag_chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "    \n",
    "    def _get_hallucination_grader(self):\n",
    "        ### Hallucination Grader\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "\n",
    "        # Prompt\n",
    "        system = \"\"\"Ты занимаешься оцениванием, насколько LLM-вывод обоснован / подкреплен набором полученных фактов. \\n \n",
    "             В качестве ответа ты выдаешь бинарную оценку «да» или «нет». Да» означает, что ответ обоснован / подкреплен набором фактов, а имена собственные совпадают / не перепутаны с изначально упомянутыми в вопросе.\"\"\"\n",
    "        hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", \"Набор фактов: \\n\\n {documents} \\n\\n Ответ LLM: {generation}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.hallucination_grader = hallucination_prompt | llm\n",
    "    \n",
    "    \n",
    "    def _get_answer_grader(self):\n",
    "        ### Answer Grader\n",
    "        # LLM with function call\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "        # Prompt\n",
    "        system = \"\"\"Ты оцениваешь, насколько ответ LLM соответствует/решает вопрос \\n \n",
    "             Дай бинарную оценку «да» или «нет». Да» означает, что ответ разрешает вопрос. Спасибо!\"\"\"\n",
    "        \n",
    "        answer_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", \"Вопрос пользователя: \\n\\n {question} \\n\\n Ответ LLM: {generation}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.answer_grader = answer_prompt | llm\n",
    "\n",
    "        \n",
    "    def _get_question_writer(self):\n",
    "        ### Question Re-writer\n",
    "        llm = ChatYandexGPT(iam_token=self.iam_token, folder_id=self.gpt_folder_id, temperature=0, sleep_interval=0.1)\n",
    "\n",
    "        # Prompt\n",
    "        system = \"\"\"Ты преобразуешь вопрос в лучшую версию, оптимизированную \\n \n",
    "             для поиска в векторном хранилище. Посмотри на входные данные и попытайся определить семантическое намерение / смысл.\"\"\"\n",
    "        re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Вот начальный вопрос: \\n\\n {question} \\n Сформулируй улучшенный вопрос.\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    \n",
    "    ### Nodes\n",
    "    def _retrieve(self, state):\n",
    "        \"\"\"\n",
    "        Retrieve documents\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): New key added to state, documents, that contains retrieved documents\n",
    "        \"\"\"\n",
    "        print(\"---RETRIEVE---\")\n",
    "        question = state[\"question\"]\n",
    "\n",
    "        # Retrieval\n",
    "        documents = self.retriever.get_relevant_documents(question) # our database used here\n",
    "        return {\"documents\": documents, \"question\": question}\n",
    "    \n",
    "    \n",
    "    def _generate(self, state):\n",
    "        \"\"\"\n",
    "        Generate LLM response based on query + documents.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): New key added to state, generation, that contains final generation\n",
    "        \"\"\"\n",
    "        print(\"---GENERATE---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # RAG generation\n",
    "        generation = self.rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "        time.sleep(1)\n",
    "        return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    \n",
    "    \n",
    "    def _grade_documents(self, state):\n",
    "        \"\"\"\n",
    "        Score retrieved documents\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): Existing key in state, documents, that now has graded documents\n",
    "        \"\"\"\n",
    "        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # Score each doc\n",
    "        filtered_docs = []\n",
    "        for d in documents:\n",
    "            score = self.retrieval_grader.invoke(\n",
    "                {\"document\": d.page_content, \"question\": question}\n",
    "            )\n",
    "            time.sleep(1)\n",
    "            grade = score.content.lower()\n",
    "            if \"да\" in grade:\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        return {\"documents\": filtered_docs, \"question\": question}\n",
    "    \n",
    "    \n",
    "    def _grade_generation_v_documents_and_question(self, state):\n",
    "        \"\"\"\n",
    "        Score generation\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): decision: if useful or not useful\n",
    "        \"\"\"\n",
    "        print(\"---CHECK HALLUCINATIONS---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "        generation = state[\"generation\"]\n",
    "\n",
    "        score = self.hallucination_grader.invoke(\n",
    "            {\"documents\": documents, \"generation\": generation}\n",
    "        )\n",
    "        time.sleep(1)\n",
    "        grade = score.content.lower()\n",
    "\n",
    "        # Check hallucination\n",
    "        if \"да\" in grade:\n",
    "            print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "            # Check question-answering\n",
    "            print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "            score = self.answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "            time.sleep(1)\n",
    "            grade = score.content.lower()\n",
    "            if \"да\" in grade:\n",
    "                print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "                return \"useful\"\n",
    "            else:\n",
    "                print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "                return \"not useful\"\n",
    "        else:\n",
    "            pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "            return \"not supported\"\n",
    "    \n",
    "    \n",
    "    def _transform_query(self, state):\n",
    "        \"\"\"\n",
    "        Re-write query.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            state (dict): New key in state, question, that now has the transformed query\n",
    "        \"\"\"\n",
    "        print(\"---TRANSFORM QUERY---\")\n",
    "        question = state[\"question\"]\n",
    "        documents = state[\"documents\"]\n",
    "\n",
    "        # Re-write question\n",
    "        better_question = self.question_rewriter.invoke({\"question\": question})\n",
    "        time.sleep(1)\n",
    "        print(f\"--- REWRITED QUESTION: {better_question} ---\")\n",
    "        return {\"documents\": documents, \"question\": better_question}\n",
    "    \n",
    "    \n",
    "    def _decide_to_generate(self, state):\n",
    "        \"\"\"\n",
    "        Decide if query should be transformed or generate LLM response.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The current graph state\n",
    "\n",
    "        Returns:\n",
    "            str: Edge to follow, \"generate\" or \"transform_query\"\n",
    "        \"\"\"\n",
    "        print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "        state[\"question\"]\n",
    "        filtered_documents = state[\"documents\"]\n",
    "\n",
    "        if not filtered_documents:\n",
    "            # All documents have been filtered check_relevance\n",
    "            # We will re-generate a new query\n",
    "            print(\n",
    "                \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "            )\n",
    "            return \"transform_query\"\n",
    "        else:\n",
    "            # We have relevant documents, so generate answer\n",
    "            print(\"---DECISION: GENERATE---\")\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b853311-b16b-47c8-b5a6-7c4ece83d6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T07:16:35.119685Z",
     "iopub.status.busy": "2024-09-13T07:16:35.118685Z",
     "iopub.status.idle": "2024-09-13T07:17:28.930526Z",
     "shell.execute_reply": "2024-09-13T07:17:28.929619Z",
     "shell.execute_reply.started": "2024-09-13T07:16:35.119644Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reflective_rag = SelfReflectiveRag(\n",
    "    db_path = database_path, db_name = filename, gpt_folder_id = folder_id, iam_token = iam_token\n",
    ")\n",
    "\n",
    "questions =  [\n",
    "        \"Чем обновлять школы округа?\"\n",
    "    ]\n",
    "\n",
    "value = reflective_rag.run_rag(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "530111c8-2e18-4f03-9136-65ac7f7c88ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T07:18:07.947633Z",
     "iopub.status.busy": "2024-09-13T07:18:07.946625Z",
     "iopub.status.idle": "2024-09-13T07:18:07.958503Z",
     "shell.execute_reply": "2024-09-13T07:18:07.957874Z",
     "shell.execute_reply.started": "2024-09-13T07:18:07.947580Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = value['question']\n",
    "generation = value['generation']\n",
    "documents = value['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9158f8f-f04a-4541-a9a2-cc0161ba4cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:00:20.894439Z",
     "iopub.status.busy": "2024-09-13T08:00:20.893569Z",
     "iopub.status.idle": "2024-09-13T08:00:20.912215Z",
     "shell.execute_reply": "2024-09-13T08:00:20.911518Z",
     "shell.execute_reply.started": "2024-09-13T08:00:20.894412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Извлечение содержимого page_content из объектов Document\n",
    "page_contents = [doc.page_content for doc in documents]\n",
    "\n",
    "# Создание DataFrame\n",
    "df = pd.DataFrame(page_contents, columns=['page_content'])\n",
    "\n",
    "# Вывод DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34d2aaff-08cf-4fc6-9174-a84bf2950bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:05:00.584184Z",
     "iopub.status.busy": "2024-09-13T08:05:00.583459Z",
     "iopub.status.idle": "2024-09-13T08:05:00.609852Z",
     "shell.execute_reply": "2024-09-13T08:05:00.609080Z",
     "shell.execute_reply.started": "2024-09-13T08:05:00.584159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Добавление колонок question и generation\n",
    "df['question'] = question\n",
    "df['answer'] = generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25b7518f-a277-4e0f-9ca6-9170979b910f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:06:17.800427Z",
     "iopub.status.busy": "2024-09-13T08:06:17.799401Z",
     "iopub.status.idle": "2024-09-13T08:06:17.812757Z",
     "shell.execute_reply": "2024-09-13T08:06:17.811889Z",
     "shell.execute_reply.started": "2024-09-13T08:06:17.800395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'page_content': 'contexts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cf248-4aa8-456a-8525-0f3f8930c332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:06:18.801876Z",
     "iopub.status.busy": "2024-09-13T08:06:18.800862Z",
     "iopub.status.idle": "2024-09-13T08:06:18.821747Z",
     "shell.execute_reply": "2024-09-13T08:06:18.821074Z",
     "shell.execute_reply.started": "2024-09-13T08:06:18.801842Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba071a1c-a704-426c-93fc-95124c4ce064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T08:09:15.846937Z",
     "iopub.status.busy": "2024-09-13T08:09:15.845911Z",
     "iopub.status.idle": "2024-09-13T08:09:16.833952Z",
     "shell.execute_reply": "2024-09-13T08:09:16.833094Z",
     "shell.execute_reply.started": "2024-09-13T08:09:15.846895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Сохранение DataFrame в файл Excel\n",
    "output_file = '/home/jupyter/datasphere/s3/hack-object-storage/database/file_for_merge.xlsx'\n",
    "df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595551bb-967b-4e4a-8be1-5e8bfaec5097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T22:03:28.658150Z",
     "iopub.status.busy": "2024-09-12T22:03:28.657106Z",
     "iopub.status.idle": "2024-09-12T22:03:29.952518Z",
     "shell.execute_reply": "2024-09-12T22:03:29.951459Z",
     "shell.execute_reply.started": "2024-09-12T22:03:28.658095Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
